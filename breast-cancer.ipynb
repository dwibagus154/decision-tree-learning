{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eca90635-0662-4d2e-91e3-7dd4117ab803",
   "metadata": {},
   "source": [
    "# Tugas Kecil 1 IF3270 Pembelajaran Mesin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b109e0f-1017-48e0-80fc-c79c61ef02c9",
   "metadata": {},
   "source": [
    "### Anggota Kelompok\n",
    "### - 13519057 Kadek Dwi Bagus\n",
    "### - 13519217 Hughie Alghaniyyu Emiliano"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7b0ca6-7dcb-4d3c-9c6d-069b570354b4",
   "metadata": {},
   "source": [
    "#### Import Dataset Breast Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c787477-08c8-4a2e-8e50-a34076910b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>malignant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>malignant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>malignant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>malignant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>malignant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>malignant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>malignant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>malignant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>malignant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension    Species  \n",
       "0                  0.2654          0.4601                  0.11890  malignant  \n",
       "1                  0.1860          0.2750                  0.08902  malignant  \n",
       "2                  0.2430          0.3613                  0.08758  malignant  \n",
       "3                  0.2575          0.6638                  0.17300  malignant  \n",
       "4                  0.1625          0.2364                  0.07678  malignant  \n",
       "..                    ...             ...                      ...        ...  \n",
       "564                0.2216          0.2060                  0.07115  malignant  \n",
       "565                0.1628          0.2572                  0.06637  malignant  \n",
       "566                0.1418          0.2218                  0.07820  malignant  \n",
       "567                0.2650          0.4087                  0.12400  malignant  \n",
       "568                0.0000          0.2871                  0.07039     benign  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast_cancer_data = load_breast_cancer()\n",
    "df = pd.DataFrame(breast_cancer_data.data, columns=breast_cancer_data.feature_names).assign(Species=breast_cancer_data['target_names'][breast_cancer_data.target])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851cda95-7afc-4dc9-8ea9-34c88a9f8202",
   "metadata": {},
   "source": [
    "#### Split Dataset to 80% for Training and 20% for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a3aee46-69a7-46df-9998-1b2fc10141ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of training data : 455 (79.96%)\n",
      "Amount of testing data  : 114 (20.04%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.DataFrame(breast_cancer_data.data, columns=breast_cancer_data.feature_names)\n",
    "result = breast_cancer_data.target\n",
    "train_data, test_data, train_result, test_result = train_test_split(data, result, test_size=0.20)\n",
    "\n",
    "train_percentage = round((train_data.shape[0] / (train_data.shape[0] + test_data.shape[0])) * 100, 2)\n",
    "print(f\"Amount of training data : {train_data.shape[0]} ({train_percentage}%)\")\n",
    "\n",
    "test_percentage = round((test_data.shape[0] / (train_data.shape[0] + test_data.shape[0])) * 100, 2)\n",
    "print(f\"Amount of testing data  : {test_data.shape[0]} ({test_percentage}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fd4e22-5e49-40fa-924a-46c0e71ec100",
   "metadata": {},
   "source": [
    "##### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b61e6e14-c70b-42c4-a32d-38ecbd949572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>13.710</td>\n",
       "      <td>18.68</td>\n",
       "      <td>88.73</td>\n",
       "      <td>571.0</td>\n",
       "      <td>0.09916</td>\n",
       "      <td>0.10700</td>\n",
       "      <td>0.053850</td>\n",
       "      <td>0.037830</td>\n",
       "      <td>0.1714</td>\n",
       "      <td>0.06843</td>\n",
       "      <td>...</td>\n",
       "      <td>15.110</td>\n",
       "      <td>25.63</td>\n",
       "      <td>99.43</td>\n",
       "      <td>701.9</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.25660</td>\n",
       "      <td>0.193500</td>\n",
       "      <td>0.12840</td>\n",
       "      <td>0.2849</td>\n",
       "      <td>0.09031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>12.040</td>\n",
       "      <td>28.14</td>\n",
       "      <td>76.85</td>\n",
       "      <td>449.9</td>\n",
       "      <td>0.08752</td>\n",
       "      <td>0.06000</td>\n",
       "      <td>0.023670</td>\n",
       "      <td>0.023770</td>\n",
       "      <td>0.1854</td>\n",
       "      <td>0.05698</td>\n",
       "      <td>...</td>\n",
       "      <td>13.600</td>\n",
       "      <td>33.33</td>\n",
       "      <td>87.24</td>\n",
       "      <td>567.6</td>\n",
       "      <td>0.10410</td>\n",
       "      <td>0.09726</td>\n",
       "      <td>0.055240</td>\n",
       "      <td>0.05547</td>\n",
       "      <td>0.2404</td>\n",
       "      <td>0.06639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>8.571</td>\n",
       "      <td>13.10</td>\n",
       "      <td>54.53</td>\n",
       "      <td>221.3</td>\n",
       "      <td>0.10360</td>\n",
       "      <td>0.07632</td>\n",
       "      <td>0.025650</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.1678</td>\n",
       "      <td>0.07126</td>\n",
       "      <td>...</td>\n",
       "      <td>9.473</td>\n",
       "      <td>18.45</td>\n",
       "      <td>63.30</td>\n",
       "      <td>275.6</td>\n",
       "      <td>0.16410</td>\n",
       "      <td>0.22350</td>\n",
       "      <td>0.175400</td>\n",
       "      <td>0.08512</td>\n",
       "      <td>0.2983</td>\n",
       "      <td>0.10490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>11.710</td>\n",
       "      <td>15.45</td>\n",
       "      <td>75.03</td>\n",
       "      <td>420.3</td>\n",
       "      <td>0.11500</td>\n",
       "      <td>0.07281</td>\n",
       "      <td>0.040060</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.2009</td>\n",
       "      <td>0.06506</td>\n",
       "      <td>...</td>\n",
       "      <td>13.060</td>\n",
       "      <td>18.16</td>\n",
       "      <td>84.16</td>\n",
       "      <td>516.4</td>\n",
       "      <td>0.14600</td>\n",
       "      <td>0.11150</td>\n",
       "      <td>0.108700</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.2765</td>\n",
       "      <td>0.07806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>12.830</td>\n",
       "      <td>15.73</td>\n",
       "      <td>82.89</td>\n",
       "      <td>506.9</td>\n",
       "      <td>0.09040</td>\n",
       "      <td>0.08269</td>\n",
       "      <td>0.058350</td>\n",
       "      <td>0.030780</td>\n",
       "      <td>0.1705</td>\n",
       "      <td>0.05913</td>\n",
       "      <td>...</td>\n",
       "      <td>14.090</td>\n",
       "      <td>19.35</td>\n",
       "      <td>93.22</td>\n",
       "      <td>605.8</td>\n",
       "      <td>0.13260</td>\n",
       "      <td>0.26100</td>\n",
       "      <td>0.347600</td>\n",
       "      <td>0.09783</td>\n",
       "      <td>0.3006</td>\n",
       "      <td>0.07802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>11.840</td>\n",
       "      <td>18.70</td>\n",
       "      <td>77.93</td>\n",
       "      <td>440.6</td>\n",
       "      <td>0.11090</td>\n",
       "      <td>0.15160</td>\n",
       "      <td>0.121800</td>\n",
       "      <td>0.051820</td>\n",
       "      <td>0.2301</td>\n",
       "      <td>0.07799</td>\n",
       "      <td>...</td>\n",
       "      <td>16.820</td>\n",
       "      <td>28.12</td>\n",
       "      <td>119.40</td>\n",
       "      <td>888.7</td>\n",
       "      <td>0.16370</td>\n",
       "      <td>0.57750</td>\n",
       "      <td>0.695600</td>\n",
       "      <td>0.15460</td>\n",
       "      <td>0.4761</td>\n",
       "      <td>0.14020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>13.110</td>\n",
       "      <td>22.54</td>\n",
       "      <td>87.02</td>\n",
       "      <td>529.4</td>\n",
       "      <td>0.10020</td>\n",
       "      <td>0.14830</td>\n",
       "      <td>0.087050</td>\n",
       "      <td>0.051020</td>\n",
       "      <td>0.1850</td>\n",
       "      <td>0.07310</td>\n",
       "      <td>...</td>\n",
       "      <td>14.550</td>\n",
       "      <td>29.16</td>\n",
       "      <td>99.48</td>\n",
       "      <td>639.3</td>\n",
       "      <td>0.13490</td>\n",
       "      <td>0.44020</td>\n",
       "      <td>0.316200</td>\n",
       "      <td>0.11260</td>\n",
       "      <td>0.4128</td>\n",
       "      <td>0.10760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>11.330</td>\n",
       "      <td>14.16</td>\n",
       "      <td>71.79</td>\n",
       "      <td>396.6</td>\n",
       "      <td>0.09379</td>\n",
       "      <td>0.03872</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.1954</td>\n",
       "      <td>0.05821</td>\n",
       "      <td>...</td>\n",
       "      <td>12.200</td>\n",
       "      <td>18.99</td>\n",
       "      <td>77.37</td>\n",
       "      <td>458.0</td>\n",
       "      <td>0.12590</td>\n",
       "      <td>0.07348</td>\n",
       "      <td>0.004955</td>\n",
       "      <td>0.01111</td>\n",
       "      <td>0.2758</td>\n",
       "      <td>0.06386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>11.600</td>\n",
       "      <td>24.49</td>\n",
       "      <td>74.23</td>\n",
       "      <td>417.2</td>\n",
       "      <td>0.07474</td>\n",
       "      <td>0.05688</td>\n",
       "      <td>0.019740</td>\n",
       "      <td>0.013130</td>\n",
       "      <td>0.1935</td>\n",
       "      <td>0.05878</td>\n",
       "      <td>...</td>\n",
       "      <td>12.440</td>\n",
       "      <td>31.62</td>\n",
       "      <td>81.39</td>\n",
       "      <td>476.5</td>\n",
       "      <td>0.09545</td>\n",
       "      <td>0.13610</td>\n",
       "      <td>0.072390</td>\n",
       "      <td>0.04815</td>\n",
       "      <td>0.3244</td>\n",
       "      <td>0.06745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>19.800</td>\n",
       "      <td>21.56</td>\n",
       "      <td>129.70</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>0.09383</td>\n",
       "      <td>0.13060</td>\n",
       "      <td>0.127200</td>\n",
       "      <td>0.086910</td>\n",
       "      <td>0.2094</td>\n",
       "      <td>0.05581</td>\n",
       "      <td>...</td>\n",
       "      <td>25.730</td>\n",
       "      <td>28.64</td>\n",
       "      <td>170.30</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0.13530</td>\n",
       "      <td>0.32350</td>\n",
       "      <td>0.361700</td>\n",
       "      <td>0.18200</td>\n",
       "      <td>0.3070</td>\n",
       "      <td>0.08255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>455 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "523       13.710         18.68           88.73      571.0          0.09916   \n",
       "471       12.040         28.14           76.85      449.9          0.08752   \n",
       "525        8.571         13.10           54.53      221.3          0.10360   \n",
       "344       11.710         15.45           75.03      420.3          0.11500   \n",
       "475       12.830         15.73           82.89      506.9          0.09040   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "31        11.840         18.70           77.93      440.6          0.11090   \n",
       "208       13.110         22.54           87.02      529.4          0.10020   \n",
       "276       11.330         14.16           71.79      396.6          0.09379   \n",
       "305       11.600         24.49           74.23      417.2          0.07474   \n",
       "218       19.800         21.56          129.70     1230.0          0.09383   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "523           0.10700        0.053850             0.037830         0.1714   \n",
       "471           0.06000        0.023670             0.023770         0.1854   \n",
       "525           0.07632        0.025650             0.015100         0.1678   \n",
       "344           0.07281        0.040060             0.032500         0.2009   \n",
       "475           0.08269        0.058350             0.030780         0.1705   \n",
       "..                ...             ...                  ...            ...   \n",
       "31            0.15160        0.121800             0.051820         0.2301   \n",
       "208           0.14830        0.087050             0.051020         0.1850   \n",
       "276           0.03872        0.001487             0.003333         0.1954   \n",
       "305           0.05688        0.019740             0.013130         0.1935   \n",
       "218           0.13060        0.127200             0.086910         0.2094   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "523                 0.06843  ...        15.110          25.63   \n",
       "471                 0.05698  ...        13.600          33.33   \n",
       "525                 0.07126  ...         9.473          18.45   \n",
       "344                 0.06506  ...        13.060          18.16   \n",
       "475                 0.05913  ...        14.090          19.35   \n",
       "..                      ...  ...           ...            ...   \n",
       "31                  0.07799  ...        16.820          28.12   \n",
       "208                 0.07310  ...        14.550          29.16   \n",
       "276                 0.05821  ...        12.200          18.99   \n",
       "305                 0.05878  ...        12.440          31.62   \n",
       "218                 0.05581  ...        25.730          28.64   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "523            99.43       701.9           0.14250            0.25660   \n",
       "471            87.24       567.6           0.10410            0.09726   \n",
       "525            63.30       275.6           0.16410            0.22350   \n",
       "344            84.16       516.4           0.14600            0.11150   \n",
       "475            93.22       605.8           0.13260            0.26100   \n",
       "..               ...         ...               ...                ...   \n",
       "31            119.40       888.7           0.16370            0.57750   \n",
       "208            99.48       639.3           0.13490            0.44020   \n",
       "276            77.37       458.0           0.12590            0.07348   \n",
       "305            81.39       476.5           0.09545            0.13610   \n",
       "218           170.30      2009.0           0.13530            0.32350   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "523         0.193500               0.12840          0.2849   \n",
       "471         0.055240               0.05547          0.2404   \n",
       "525         0.175400               0.08512          0.2983   \n",
       "344         0.108700               0.07864          0.2765   \n",
       "475         0.347600               0.09783          0.3006   \n",
       "..               ...                   ...             ...   \n",
       "31          0.695600               0.15460          0.4761   \n",
       "208         0.316200               0.11260          0.4128   \n",
       "276         0.004955               0.01111          0.2758   \n",
       "305         0.072390               0.04815          0.3244   \n",
       "218         0.361700               0.18200          0.3070   \n",
       "\n",
       "     worst fractal dimension  \n",
       "523                  0.09031  \n",
       "471                  0.06639  \n",
       "525                  0.10490  \n",
       "344                  0.07806  \n",
       "475                  0.07802  \n",
       "..                       ...  \n",
       "31                   0.14020  \n",
       "208                  0.10760  \n",
       "276                  0.06386  \n",
       "305                  0.06745  \n",
       "218                  0.08255  \n",
       "\n",
       "[455 rows x 30 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e645317-4915-4aba-a254-b6ad4c7b7f5f",
   "metadata": {},
   "source": [
    "##### Training Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffe78fa5-c038-4a12-9efc-94b3fcf2fda3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e461108-bf31-4416-95f8-db3138fcc0dc",
   "metadata": {},
   "source": [
    "##### Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df4e3350-8a66-4f85-a16e-caf2caef880c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>13.69</td>\n",
       "      <td>16.07</td>\n",
       "      <td>87.84</td>\n",
       "      <td>579.1</td>\n",
       "      <td>0.08302</td>\n",
       "      <td>0.06374</td>\n",
       "      <td>0.02556</td>\n",
       "      <td>0.02031</td>\n",
       "      <td>0.1872</td>\n",
       "      <td>0.05669</td>\n",
       "      <td>...</td>\n",
       "      <td>14.84</td>\n",
       "      <td>20.21</td>\n",
       "      <td>99.16</td>\n",
       "      <td>670.6</td>\n",
       "      <td>0.11050</td>\n",
       "      <td>0.2096</td>\n",
       "      <td>0.1346</td>\n",
       "      <td>0.06987</td>\n",
       "      <td>0.3323</td>\n",
       "      <td>0.07701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>18.94</td>\n",
       "      <td>21.31</td>\n",
       "      <td>123.60</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>0.09009</td>\n",
       "      <td>0.10290</td>\n",
       "      <td>0.10800</td>\n",
       "      <td>0.07951</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.05461</td>\n",
       "      <td>...</td>\n",
       "      <td>24.86</td>\n",
       "      <td>26.58</td>\n",
       "      <td>165.90</td>\n",
       "      <td>1866.0</td>\n",
       "      <td>0.11930</td>\n",
       "      <td>0.2336</td>\n",
       "      <td>0.2687</td>\n",
       "      <td>0.17890</td>\n",
       "      <td>0.2551</td>\n",
       "      <td>0.06589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>12.47</td>\n",
       "      <td>18.60</td>\n",
       "      <td>81.09</td>\n",
       "      <td>481.9</td>\n",
       "      <td>0.09965</td>\n",
       "      <td>0.10580</td>\n",
       "      <td>0.08005</td>\n",
       "      <td>0.03821</td>\n",
       "      <td>0.1925</td>\n",
       "      <td>0.06373</td>\n",
       "      <td>...</td>\n",
       "      <td>14.97</td>\n",
       "      <td>24.64</td>\n",
       "      <td>96.05</td>\n",
       "      <td>677.9</td>\n",
       "      <td>0.14260</td>\n",
       "      <td>0.2378</td>\n",
       "      <td>0.2671</td>\n",
       "      <td>0.10150</td>\n",
       "      <td>0.3014</td>\n",
       "      <td>0.08750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>16.03</td>\n",
       "      <td>15.51</td>\n",
       "      <td>105.80</td>\n",
       "      <td>793.2</td>\n",
       "      <td>0.09491</td>\n",
       "      <td>0.13710</td>\n",
       "      <td>0.12040</td>\n",
       "      <td>0.07041</td>\n",
       "      <td>0.1782</td>\n",
       "      <td>0.05976</td>\n",
       "      <td>...</td>\n",
       "      <td>18.76</td>\n",
       "      <td>21.98</td>\n",
       "      <td>124.30</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>0.14350</td>\n",
       "      <td>0.4478</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>0.19810</td>\n",
       "      <td>0.3019</td>\n",
       "      <td>0.09124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>14.95</td>\n",
       "      <td>18.77</td>\n",
       "      <td>97.84</td>\n",
       "      <td>689.5</td>\n",
       "      <td>0.08138</td>\n",
       "      <td>0.11670</td>\n",
       "      <td>0.09050</td>\n",
       "      <td>0.03562</td>\n",
       "      <td>0.1744</td>\n",
       "      <td>0.06493</td>\n",
       "      <td>...</td>\n",
       "      <td>16.25</td>\n",
       "      <td>25.47</td>\n",
       "      <td>107.10</td>\n",
       "      <td>809.7</td>\n",
       "      <td>0.09970</td>\n",
       "      <td>0.2521</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.08405</td>\n",
       "      <td>0.2852</td>\n",
       "      <td>0.09218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>13.11</td>\n",
       "      <td>15.56</td>\n",
       "      <td>87.21</td>\n",
       "      <td>530.2</td>\n",
       "      <td>0.13980</td>\n",
       "      <td>0.17650</td>\n",
       "      <td>0.20710</td>\n",
       "      <td>0.09601</td>\n",
       "      <td>0.1925</td>\n",
       "      <td>0.07692</td>\n",
       "      <td>...</td>\n",
       "      <td>16.31</td>\n",
       "      <td>22.40</td>\n",
       "      <td>106.40</td>\n",
       "      <td>827.2</td>\n",
       "      <td>0.18620</td>\n",
       "      <td>0.4099</td>\n",
       "      <td>0.6376</td>\n",
       "      <td>0.19860</td>\n",
       "      <td>0.3147</td>\n",
       "      <td>0.14050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>19.00</td>\n",
       "      <td>18.91</td>\n",
       "      <td>123.40</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>0.08217</td>\n",
       "      <td>0.08028</td>\n",
       "      <td>0.09271</td>\n",
       "      <td>0.05627</td>\n",
       "      <td>0.1946</td>\n",
       "      <td>0.05044</td>\n",
       "      <td>...</td>\n",
       "      <td>22.32</td>\n",
       "      <td>25.73</td>\n",
       "      <td>148.20</td>\n",
       "      <td>1538.0</td>\n",
       "      <td>0.10210</td>\n",
       "      <td>0.2264</td>\n",
       "      <td>0.3207</td>\n",
       "      <td>0.12180</td>\n",
       "      <td>0.2841</td>\n",
       "      <td>0.06541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>13.74</td>\n",
       "      <td>17.91</td>\n",
       "      <td>88.12</td>\n",
       "      <td>585.0</td>\n",
       "      <td>0.07944</td>\n",
       "      <td>0.06376</td>\n",
       "      <td>0.02881</td>\n",
       "      <td>0.01329</td>\n",
       "      <td>0.1473</td>\n",
       "      <td>0.05580</td>\n",
       "      <td>...</td>\n",
       "      <td>15.34</td>\n",
       "      <td>22.46</td>\n",
       "      <td>97.19</td>\n",
       "      <td>725.9</td>\n",
       "      <td>0.09711</td>\n",
       "      <td>0.1824</td>\n",
       "      <td>0.1564</td>\n",
       "      <td>0.06019</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.07014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>13.21</td>\n",
       "      <td>25.25</td>\n",
       "      <td>84.10</td>\n",
       "      <td>537.9</td>\n",
       "      <td>0.08791</td>\n",
       "      <td>0.05205</td>\n",
       "      <td>0.02772</td>\n",
       "      <td>0.02068</td>\n",
       "      <td>0.1619</td>\n",
       "      <td>0.05584</td>\n",
       "      <td>...</td>\n",
       "      <td>14.35</td>\n",
       "      <td>34.23</td>\n",
       "      <td>91.29</td>\n",
       "      <td>632.9</td>\n",
       "      <td>0.12890</td>\n",
       "      <td>0.1063</td>\n",
       "      <td>0.1390</td>\n",
       "      <td>0.06005</td>\n",
       "      <td>0.2444</td>\n",
       "      <td>0.06788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>13.90</td>\n",
       "      <td>16.62</td>\n",
       "      <td>88.97</td>\n",
       "      <td>599.4</td>\n",
       "      <td>0.06828</td>\n",
       "      <td>0.05319</td>\n",
       "      <td>0.02224</td>\n",
       "      <td>0.01339</td>\n",
       "      <td>0.1813</td>\n",
       "      <td>0.05536</td>\n",
       "      <td>...</td>\n",
       "      <td>15.14</td>\n",
       "      <td>21.80</td>\n",
       "      <td>101.20</td>\n",
       "      <td>718.9</td>\n",
       "      <td>0.09384</td>\n",
       "      <td>0.2006</td>\n",
       "      <td>0.1384</td>\n",
       "      <td>0.06222</td>\n",
       "      <td>0.2679</td>\n",
       "      <td>0.07698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "374        13.69         16.07           87.84      579.1          0.08302   \n",
       "70         18.94         21.31          123.60     1130.0          0.09009   \n",
       "204        12.47         18.60           81.09      481.9          0.09965   \n",
       "330        16.03         15.51          105.80      793.2          0.09491   \n",
       "147        14.95         18.77           97.84      689.5          0.08138   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "105        13.11         15.56           87.21      530.2          0.13980   \n",
       "127        19.00         18.91          123.40     1138.0          0.08217   \n",
       "149        13.74         17.91           88.12      585.0          0.07944   \n",
       "457        13.21         25.25           84.10      537.9          0.08791   \n",
       "477        13.90         16.62           88.97      599.4          0.06828   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "374           0.06374         0.02556              0.02031         0.1872   \n",
       "70            0.10290         0.10800              0.07951         0.1582   \n",
       "204           0.10580         0.08005              0.03821         0.1925   \n",
       "330           0.13710         0.12040              0.07041         0.1782   \n",
       "147           0.11670         0.09050              0.03562         0.1744   \n",
       "..                ...             ...                  ...            ...   \n",
       "105           0.17650         0.20710              0.09601         0.1925   \n",
       "127           0.08028         0.09271              0.05627         0.1946   \n",
       "149           0.06376         0.02881              0.01329         0.1473   \n",
       "457           0.05205         0.02772              0.02068         0.1619   \n",
       "477           0.05319         0.02224              0.01339         0.1813   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "374                 0.05669  ...         14.84          20.21   \n",
       "70                  0.05461  ...         24.86          26.58   \n",
       "204                 0.06373  ...         14.97          24.64   \n",
       "330                 0.05976  ...         18.76          21.98   \n",
       "147                 0.06493  ...         16.25          25.47   \n",
       "..                      ...  ...           ...            ...   \n",
       "105                 0.07692  ...         16.31          22.40   \n",
       "127                 0.05044  ...         22.32          25.73   \n",
       "149                 0.05580  ...         15.34          22.46   \n",
       "457                 0.05584  ...         14.35          34.23   \n",
       "477                 0.05536  ...         15.14          21.80   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "374            99.16       670.6           0.11050             0.2096   \n",
       "70            165.90      1866.0           0.11930             0.2336   \n",
       "204            96.05       677.9           0.14260             0.2378   \n",
       "330           124.30      1070.0           0.14350             0.4478   \n",
       "147           107.10       809.7           0.09970             0.2521   \n",
       "..               ...         ...               ...                ...   \n",
       "105           106.40       827.2           0.18620             0.4099   \n",
       "127           148.20      1538.0           0.10210             0.2264   \n",
       "149            97.19       725.9           0.09711             0.1824   \n",
       "457            91.29       632.9           0.12890             0.1063   \n",
       "477           101.20       718.9           0.09384             0.2006   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "374           0.1346               0.06987          0.3323   \n",
       "70            0.2687               0.17890          0.2551   \n",
       "204           0.2671               0.10150          0.3014   \n",
       "330           0.4956               0.19810          0.3019   \n",
       "147           0.2500               0.08405          0.2852   \n",
       "..               ...                   ...             ...   \n",
       "105           0.6376               0.19860          0.3147   \n",
       "127           0.3207               0.12180          0.2841   \n",
       "149           0.1564               0.06019          0.2350   \n",
       "457           0.1390               0.06005          0.2444   \n",
       "477           0.1384               0.06222          0.2679   \n",
       "\n",
       "     worst fractal dimension  \n",
       "374                  0.07701  \n",
       "70                   0.06589  \n",
       "204                  0.08750  \n",
       "330                  0.09124  \n",
       "147                  0.09218  \n",
       "..                       ...  \n",
       "105                  0.14050  \n",
       "127                  0.06541  \n",
       "149                  0.07014  \n",
       "457                  0.06788  \n",
       "477                  0.07698  \n",
       "\n",
       "[114 rows x 30 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188b3d02-5c59-40e3-bf35-0610b18bcde0",
   "metadata": {},
   "source": [
    "##### Testing Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a696a349-9b4a-4712-acb1-f076134618e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       0, 1, 1, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf3d750",
   "metadata": {},
   "source": [
    "#### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9039a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5738636363636364, 0.9444444444444444, 'X[22] <= 105.15\\ngini = 0.465\\nsamples = 455\\nvalue = [167, 288]'),\n",
       " Text(0.42045454545454547, 0.8333333333333334, 'X[27] <= 0.181\\ngini = 0.083\\nsamples = 277\\nvalue = [12, 265]'),\n",
       " Text(0.375, 0.7222222222222222, 'X[29] <= 0.056\\ngini = 0.05\\nsamples = 272\\nvalue = [7, 265]'),\n",
       " Text(0.32954545454545453, 0.6111111111111112, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(0.42045454545454547, 0.6111111111111112, 'X[27] <= 0.135\\ngini = 0.043\\nsamples = 271\\nvalue = [6, 265]'),\n",
       " Text(0.29545454545454547, 0.5, 'X[13] <= 48.975\\ngini = 0.023\\nsamples = 260\\nvalue = [3, 257]'),\n",
       " Text(0.18181818181818182, 0.3888888888888889, 'X[14] <= 0.003\\ngini = 0.015\\nsamples = 257\\nvalue = [2, 255]'),\n",
       " Text(0.09090909090909091, 0.2777777777777778, 'X[14] <= 0.003\\ngini = 0.32\\nsamples = 5\\nvalue = [1, 4]'),\n",
       " Text(0.045454545454545456, 0.16666666666666666, 'gini = 0.0\\nsamples = 4\\nvalue = [0, 4]'),\n",
       " Text(0.13636363636363635, 0.16666666666666666, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(0.2727272727272727, 0.2777777777777778, 'X[21] <= 33.35\\ngini = 0.008\\nsamples = 252\\nvalue = [1, 251]'),\n",
       " Text(0.22727272727272727, 0.16666666666666666, 'gini = 0.0\\nsamples = 235\\nvalue = [0, 235]'),\n",
       " Text(0.3181818181818182, 0.16666666666666666, 'X[21] <= 33.56\\ngini = 0.111\\nsamples = 17\\nvalue = [1, 16]'),\n",
       " Text(0.2727272727272727, 0.05555555555555555, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(0.36363636363636365, 0.05555555555555555, 'gini = 0.0\\nsamples = 16\\nvalue = [0, 16]'),\n",
       " Text(0.4090909090909091, 0.3888888888888889, 'X[15] <= 0.014\\ngini = 0.444\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(0.36363636363636365, 0.2777777777777778, 'gini = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(0.45454545454545453, 0.2777777777777778, 'gini = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(0.5454545454545454, 0.5, 'X[1] <= 20.785\\ngini = 0.397\\nsamples = 11\\nvalue = [3, 8]'),\n",
       " Text(0.5, 0.3888888888888889, 'gini = 0.0\\nsamples = 8\\nvalue = [0, 8]'),\n",
       " Text(0.5909090909090909, 0.3888888888888889, 'gini = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(0.4659090909090909, 0.7222222222222222, 'gini = 0.0\\nsamples = 5\\nvalue = [5, 0]'),\n",
       " Text(0.7272727272727273, 0.8333333333333334, 'X[7] <= 0.049\\ngini = 0.225\\nsamples = 178\\nvalue = [155, 23]'),\n",
       " Text(0.6363636363636364, 0.7222222222222222, 'X[21] <= 26.375\\ngini = 0.477\\nsamples = 28\\nvalue = [11, 17]'),\n",
       " Text(0.5909090909090909, 0.6111111111111112, 'gini = 0.0\\nsamples = 13\\nvalue = [0, 13]'),\n",
       " Text(0.6818181818181818, 0.6111111111111112, 'X[15] <= 0.018\\ngini = 0.391\\nsamples = 15\\nvalue = [11, 4]'),\n",
       " Text(0.6363636363636364, 0.5, 'gini = 0.0\\nsamples = 11\\nvalue = [11, 0]'),\n",
       " Text(0.7272727272727273, 0.5, 'gini = 0.0\\nsamples = 4\\nvalue = [0, 4]'),\n",
       " Text(0.8181818181818182, 0.7222222222222222, 'X[26] <= 0.216\\ngini = 0.077\\nsamples = 150\\nvalue = [144, 6]'),\n",
       " Text(0.7727272727272727, 0.6111111111111112, 'gini = 0.0\\nsamples = 4\\nvalue = [0, 4]'),\n",
       " Text(0.8636363636363636, 0.6111111111111112, 'X[21] <= 15.435\\ngini = 0.027\\nsamples = 146\\nvalue = [144, 2]'),\n",
       " Text(0.8181818181818182, 0.5, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(0.9090909090909091, 0.5, 'X[1] <= 15.485\\ngini = 0.014\\nsamples = 145\\nvalue = [144, 1]'),\n",
       " Text(0.8636363636363636, 0.3888888888888889, 'X[8] <= 0.177\\ngini = 0.278\\nsamples = 6\\nvalue = [5, 1]'),\n",
       " Text(0.8181818181818182, 0.2777777777777778, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(0.9090909090909091, 0.2777777777777778, 'gini = 0.0\\nsamples = 5\\nvalue = [5, 0]'),\n",
       " Text(0.9545454545454546, 0.3888888888888889, 'gini = 0.0\\nsamples = 139\\nvalue = [139, 0]')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "clfDTC = tree.DecisionTreeClassifier()\n",
    "clfDTC = clfDTC.fit(train_data, train_result)\n",
    "tree.plot_tree(clfDTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0344296b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40  5]\n",
      " [ 6 63]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "prediction = clfDTC.predict(test_data)\n",
    "print(metrics.confusion_matrix(test_result, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78b90998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.89      0.88        45\n",
      "           1       0.93      0.91      0.92        69\n",
      "\n",
      "    accuracy                           0.90       114\n",
      "   macro avg       0.90      0.90      0.90       114\n",
      "weighted avg       0.90      0.90      0.90       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_result, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f3a6598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy     : 0.9035087719298246\n",
      "Precision    : 0.9264705882352942\n",
      "Recall       : 0.9130434782608695\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy     : {metrics.accuracy_score(test_result, prediction)}\")\n",
    "print(f\"Precision    : {metrics.precision_score(test_result, prediction)}\")\n",
    "print(f\"Recall       : {metrics.recall_score(test_result, prediction)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315422d9",
   "metadata": {},
   "source": [
    "##### Analisis\n",
    "Dari hasil diatas, dapat disimpulkan bawah algoritma Decision Tree Classifier memiliki accuracy dan F1 Score 90% sehingga dari 114 instance yang digunakan sebagai data test, terdapat sekitar 12 data yang missclasified. \n",
    "\n",
    "Hasil ini juga bergantung dengan pembagian data train dan data test yang dirandom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14ee407",
   "metadata": {},
   "source": [
    "#### ID3 ESTIMATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b9c044bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from id3.id3 import Id3Estimator\n",
    "estimator = Id3Estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af26cf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = estimator.fit(train_data, train_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10da9332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42  3]\n",
      " [ 6 63]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90        45\n",
      "           1       0.95      0.91      0.93        69\n",
      "\n",
      "    accuracy                           0.92       114\n",
      "   macro avg       0.91      0.92      0.92       114\n",
      "weighted avg       0.92      0.92      0.92       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = estimator.predict(test_data)\n",
    "print(metrics.confusion_matrix(test_result, prediction))\n",
    "print(metrics.classification_report(test_result, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "915f9c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy     : 0.9210526315789473\n",
      "Precision    : 0.9545454545454546\n",
      "Recall       : 0.9130434782608695\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy     : {metrics.accuracy_score(test_result, prediction)}\")\n",
    "print(f\"Precision    : {metrics.precision_score(test_result, prediction)}\")\n",
    "print(f\"Recall       : {metrics.recall_score(test_result, prediction)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca744fdf",
   "metadata": {},
   "source": [
    "##### Analisis\n",
    "Dari hasil diatas, dapat disimpulkan bawah algoritma ID3 Estimator memiliki accuracy dan F1 Score 92% sehingga dari 114 instance yang digunakan sebagai data test, terdapat sekitar 9 data yang missclasified.\n",
    "\n",
    "Hasil ini juga bergantung dengan pembagian data train dan data test yang dirandom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c683d0-122f-436b-8cea-9b7ee860db97",
   "metadata": {},
   "source": [
    "#### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf951515-865e-4aba-a985-9cd419533cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "kmeans.fit(train_data)\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0d69182-8e05-41e1-974f-1317e3dcae00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.96432632e+01, 2.15812632e+01, 1.29766316e+02, 1.21855789e+03,\n",
       "        1.00691368e-01, 1.44408947e-01, 1.73399263e-01, 1.00568632e-01,\n",
       "        1.90501053e-01, 6.01554737e-02, 7.55786316e-01, 1.23222526e+00,\n",
       "        5.33673684e+00, 9.91404211e+01, 6.57428421e-03, 3.14379579e-02,\n",
       "        4.13249474e-02, 1.60062526e-02, 2.03889474e-02, 3.94002105e-03,\n",
       "        2.40669474e+01, 2.87997895e+01, 1.60447368e+02, 1.80758947e+03,\n",
       "        1.39921263e-01, 3.45896421e-01, 4.38180000e-01, 1.93172211e-01,\n",
       "        3.09754737e-01, 8.53207368e-02],\n",
       "       [1.25329972e+01, 1.87726111e+01, 8.10004167e+01, 4.94754444e+02,\n",
       "        9.55053056e-02, 9.20990000e-02, 6.36868333e-02, 3.42842222e-02,\n",
       "        1.79156944e-01, 6.37260556e-02, 3.11564722e-01, 1.23331833e+00,\n",
       "        2.19837222e+00, 2.44514639e+01, 7.34020833e-03, 2.35588389e-02,\n",
       "        2.89823750e-02, 1.08025750e-02, 2.09085500e-02, 3.78130917e-03,\n",
       "        1.40334250e+01, 2.49339167e+01, 9.19590833e+01, 6.19966111e+02,\n",
       "        1.30909917e-01, 2.24886500e-01, 2.20797106e-01, 9.23465583e-02,\n",
       "        2.85537778e-01, 8.36199444e-02]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da1b5d22-e787-47e3-9cb8-dbae41c09b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12 33]\n",
      " [69  0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "prediction = kmeans.fit_predict(test_data)\n",
    "print(metrics.confusion_matrix(test_result, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9c551f37-6797-4d8d-8e7e-ccc7d552e941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.27      0.19        45\n",
      "           1       0.00      0.00      0.00        69\n",
      "\n",
      "    accuracy                           0.11       114\n",
      "   macro avg       0.07      0.13      0.10       114\n",
      "weighted avg       0.06      0.11      0.08       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_result, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1901300a-cecb-4c12-ad25-0b5fb94b86a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy     : 0.10526315789473684\n",
      "Precision    : 0.0\n",
      "Recall       : 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy     : {metrics.accuracy_score(test_result, prediction)}\")\n",
    "print(f\"Precision    : {metrics.precision_score(test_result, prediction)}\")\n",
    "print(f\"Recall       : {metrics.recall_score(test_result, prediction)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec45644",
   "metadata": {},
   "source": [
    "##### Analisis\n",
    "Dari hasil diatas, dapat disimpulkan bawah algoritma K-Means memiliki accuracy dan F1 Score 11% sehingga dari 114 instance yang digunakan sebagai data test, terdapat sekitar 101 data yang missclasified.\n",
    "\n",
    "Hasil ini juga bergantung dengan data train dan data test yang dirandom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbce2ec-447f-4670-a87b-432ffb635925",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45c37e62-992b-4278-8378-e9d99eef1d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "logreg.fit(train_data, train_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47ddf801-6f9a-46cf-9d91-a685443230e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44  1]\n",
      " [ 2 67]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "prediction=logreg.predict(test_data)\n",
    "print(metrics.confusion_matrix(test_result, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fecc397b-1300-4489-9e2a-1db453908ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97        45\n",
      "           1       0.99      0.97      0.98        69\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_result, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25e230e7-1e88-43ce-96bb-b15bb0da15fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy     : 0.9736842105263158\n",
      "Precision    : 0.9852941176470589\n",
      "Recall       : 0.9710144927536232\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy     : {metrics.accuracy_score(test_result, prediction)}\")\n",
    "print(f\"Precision    : {metrics.precision_score(test_result, prediction)}\")\n",
    "print(f\"Recall       : {metrics.recall_score(test_result, prediction)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a010ec54",
   "metadata": {},
   "source": [
    "##### Analisis\n",
    "Dari hasil diatas, dapat disimpulkan bawah algoritma Logistic Regression memiliki accuracy dan F1 Score 97% sehingga dari 114 instance yang digunakan sebagai data test, terdapat sekitar 3 data yang missclasified.\n",
    "\n",
    "Hasil ini juga bergantung dengan pembagian data train dan data test yang dirandom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc299600-a729-4981-ab38-a9d4fcc27911",
   "metadata": {},
   "source": [
    "#### Neural Network MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da354644-d8cd-4189-8343-bdb63b25d7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=300, random_state=1, solver='lbfgs')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlpc = MLPClassifier(random_state=1, max_iter=300, solver='lbfgs')\n",
    "mlpc.fit(train_data, train_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eb1ed855-b450-4de4-85cc-0c1e98917e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44  1]\n",
      " [ 2 67]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "prediction=logreg.predict(test_data)\n",
    "print(metrics.confusion_matrix(test_result, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64da07e7-1e5d-4a08-8d7b-58626ebf85cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97        45\n",
      "           1       0.99      0.97      0.98        69\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_result, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d469e18e-6fb1-462a-bf31-66b820ab7c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy     : 0.9736842105263158\n",
      "Precision    : 0.9852941176470589\n",
      "Recall       : 0.9710144927536232\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy     : {metrics.accuracy_score(test_result, prediction)}\")\n",
    "print(f\"Precision    : {metrics.precision_score(test_result, prediction)}\")\n",
    "print(f\"Recall       : {metrics.recall_score(test_result, prediction)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671c6ba2",
   "metadata": {},
   "source": [
    "##### Analisis\n",
    "Dari hasil diatas, dapat disimpulkan bawah algoritma Neural Network MLP Classifier memiliki accuracy dan F1 Score 97% sehingga dari 114 instance yang digunakan sebagai data test, terdapat sekitar 3 data yang missclasified.\n",
    "\n",
    "Hasil ini juga bergantung dengan pembagian data train dan data test yang dirandom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f99bc0",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6fe58d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm \n",
    "\n",
    "clfSVM = svm.SVC()\n",
    "clfSVM.fit(train_data, train_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51ae82e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40  5]\n",
      " [ 2 67]]\n"
     ]
    }
   ],
   "source": [
    "prediction = clfSVM.predict(test_data)\n",
    "print(metrics.confusion_matrix(test_result, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3ee312d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.89      0.92        45\n",
      "           1       0.93      0.97      0.95        69\n",
      "\n",
      "    accuracy                           0.94       114\n",
      "   macro avg       0.94      0.93      0.93       114\n",
      "weighted avg       0.94      0.94      0.94       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_result, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "834384b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy     : 0.9385964912280702\n",
      "Precision    : 0.9305555555555556\n",
      "Recall       : 0.9710144927536232\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy     : {metrics.accuracy_score(test_result, prediction)}\")\n",
    "print(f\"Precision    : {metrics.precision_score(test_result, prediction)}\")\n",
    "print(f\"Recall       : {metrics.recall_score(test_result, prediction)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329c81fe",
   "metadata": {},
   "source": [
    "##### Analisis\n",
    "Dari hasil diatas, dapat disimpulkan bawah algoritma SVM (Support Vector Machine) memiliki accuracy dan F1 Score 94% sehingga dari 114 instance yang digunakan sebagai data test, terdapat sekitar 7 data yang missclasified.\n",
    "\n",
    "Hasil ini juga bergantung dengan pembagian data train dan data test yang dirandom."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
